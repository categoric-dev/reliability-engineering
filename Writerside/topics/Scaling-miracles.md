# Scaling miracles

The trio of â€œscaling miraclesâ€ that drove the silicon age is no longer a panacea. Each one has hit a wall, and
together theyâ€™re forcing the industry to rethink what â€œhigher performanceâ€ actually means.

## Scaling Laws {collapsible="true"}

<table>
    <tr>
        <td>Scaling Law</td>
        <td>What it promised</td>
        <td>Why it matters now</td>
    </tr>
    <tr>
        <td>Dennard Scaling</td>
        <td>Power density stays constant as transistors shrink â†’ <control>higher frequency, lower voltage per gate</control>.</td>
        <td>Higher clock rates cost more cooling, more die area for thermal management, and they <control>hit the â€œpower wallâ€</control></td>
    </tr>
    <tr>
        <td>Amdahlâ€™s Law</td>
        <td><control>Speedup</control> = 1 / (s + (1â€‘s)/p). Parallelism helps only until the serial portion dominates.</td>
        <td>The â€œserial fractionâ€ shrinks slower than hardware scales, so <control>adding more cores gives diminishing returns</control>.</td>
    </tr>
    <tr>
        <td>Mooreâ€™s Law</td>
        <td><control>~2Ã— transistor count per 18â€‘24mo</control>.</td>
        <td>You canâ€™t keep adding transistors at the same cost and time, so <control>â€œmore logic for less moneyâ€ stalls</control>.</td>
    </tr>
</table>

Dennard scaling, Amdahlâ€™s Law, and Mooreâ€™s Law are not deadâ€”theyâ€™re just *muted*. The next wave of performance will come
from smarter integration, domainâ€‘specific hardware, and software that can flexibly adapt to heterogeneous resources. The
real challenge is orchestrating all of this without ballooning costs or compromising reliability.

Below is a quick snapshot of why each law is struggling and what the community is doing (or could do) to keep moving forward.

<tabs>
    <tab title="Dennard Scaling">
            <p><control>Where the wall shows up:</control> Leakage & variability explode below ~20nm; subâ€‘threshold currents dominate, so you canâ€™t simply lower VDD and keep the same dynamic power.</p>
            <p><control>How the Industry Responded:</control></p>
            <list type="decimal">
                <li><p>Multiâ€‘core (and manyâ€‘core) designs: Split workload across cores; each core runs at a lower frequency, keeping perâ€‘core power low.</p></li>
                <li><p>Dynamic voltage/frequency scaling (DVFS): CPUs would automatically lower ğ‘‰ and ğ‘“ when idle or under light load.</p></li>
                <li><p>Advanced process nodes	90nm â†’ 65nm â†’ 45nm â†’ 32nm reduced capacitance ğ¶, allowing higher ğ‘“ at lower power.</p></li>
                <li><p>Architectural changes: More efficient pipelines, outâ€‘ofâ€‘order execution, larger caches to reduce memory traffic (which is powerâ€‘heavy).</p></li>
                <li><p>Thermal design improvements: Heatâ€‘spreading dielets, improved airflow, and eventually the move to liquid cooling in highâ€‘end markets.</p></li>
            </list>
    </tab>
    <tab title="Amdahlâ€™s Law">
        <p><control>Where the wall shows up:</control> Serial bottlenecks: I/O, memory hierarchy, control logic, and algorithmic dependencies.</p>
        <p><control>How the Industry Responded:</control></p>
        <list type="decimal">
            <li><p>Heterogeneous computing â€“ mix highâ€‘performance CPUs with specialized accelerators (GPUs, TPUs, FPGAs) that can handle the parallelizable portions more efficiently while leaving serial work to the CPU.</p></li>
            <li><p>Taskâ€‘level and dataâ€‘level parallelism â€“ rewrite software libraries to expose more parallel work, e.g., vectorized kernels, multiâ€‘threaded runtimes, and domainâ€‘specific languages.</p></li>
            <li><p>Algorithmic redesign â€“ develop algorithms with lower serial fractions (e.g., divideâ€‘conquer, pipelining, locality, iterative refinement) or that are inherently parallel.</p></li>
            <li><p>Dynamic scheduling & workâ€‘stealing â€“ runtime systems that balance load across cores and reduce idle time, mitigating the impact of a serial bottleneck.</p></li>
            <li><p>Hardware support for synchronization â€“ faster interconnects, atomic operations, and coherence protocols that reduce the overhead of coordinating many cores.</p></li>
            <li><p>Approximate / probabilistic computing where perfect accuracy is unnecessary.</p></li>
            <li><p>Hardwareâ€‘software coâ€‘design to expose more parallelism at the architectural level.</p></li>
        </list>
    </tab>
    <tab title="Mooreâ€™s Law">
        <p><control>Where the wall shows up:</control> Physical limits (quantum tunneling, lithography resolution). Economic limits â€“ fabs cost > $10B; yield penalties rise.</p>
        <p><control>How the Industry Responded:</control></p>
        <list type="decimal">
            <li><p>Processâ€‘node shrink still happening (3nm, 2nm), but at a slower rate.</p></li>
            <li><p>Advanced packaging (3Dâ€‘IC, TSVs, chiplets) to pack more logic without shrinking the die.</p></li>
            <li><p>Materials science: highâ€‘k/metal gates, 2D materials, nanowire FETs.</p></li>
            <li><p>Designâ€‘automation: higher abstraction levels (HLS, highâ€‘level synthesis).</p></li>
        </list>
    </tab>
</tabs>

## Bottom line

1. **Dennard Scaling**, as a **thermal limit, drove the shift to multicore** â€“ thatâ€™s why you see â€œ4â€‘coreâ€ or â€œ8â€‘coreâ€ CPUs even at modest clock speeds.
1. **Amdahlâ€™s Law** reminds us that **parallelism is powerful but not unlimited**. The serial fraction of a workload sets an upper bound on achievable speedâ€‘up, no matter how many cores we throw at it. 
1. **Mooreâ€™s Law**, as an **exponential transistorâ€‘count metric, has reached a practical plateau** due to physics, economics, and power limits. 

**The semiconductor industry is not ending but pivoting:** scaling now means integrating diverse technologies, optimizing energy efficiency, and tailoring hardware to workloads. The next wave of performance gains will come from architecture, materials, and integration rather than sheer transistor density.

The power wall forced the industry to embrace multicore designs and invest heavily in process technology. If a program is singleâ€‘threaded, adding more cores does nothing: the OS will still run that one thread on one core. To fully exploit a multicore system, the application must run multiple independent tasks simultaneously. In short, multicore CPUs set the stage: you need more parallel work to get performance gains. Async programming provides the choreography that turns I/O waits into productive CPU cycles, letting software scale cleanly across all those cores.

Asynchronous programming turns I/O waits into CPUâ€‘free time. That efficiency lets a single node handle far more traffic, which in turn reduces the number of nodes you need to provision. The smaller footprint and clear observability make horizontal scaling smoother, more predictable, and costâ€‘effective in any cloud or container environment.